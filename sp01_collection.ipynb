{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://webassets.unt.edu/assets/branding/unt-stacked-logo.svg\" alt=\"UNT | University of North Texas\" class=\"desktop-logo\" width=\"300\" height=\"500\">\n",
    "<img src=\"media/Back-to-the-future-logo.svg\" alt=\"BackToTheFuture\" class=\"desktop-logo\" width=\"600\" height=\"1000\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; justify-content: space-around; padding: 20px 40px 20px 20px; background-color: #f4f4f9; border-radius: 10px;\">\n",
    "  <!-- Team Member 1 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Sonali Sabnam</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Boss</p>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Team Member 2 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Sonam Pohuja</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Other Boss</p>\n",
    "  </div>\n",
    "\n",
    "  <!-- Team Member 3 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Luis Garcia Fuentes</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Cool Guy</p>\n",
    "  </div>\n",
    "\n",
    "  <!-- Team Member 4 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Young Yu</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Janitor</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Futures: The AI Stock Predictor\n",
    "Back to the Futures is a project where we try to do the impossible—predict the stock market with AI! <br>\n",
    "Because if there’s one thing the stock market loves, it’s being perfectly predictable.  <br>\n",
    "(spoiler: it’s not). <br>\n",
    "- Our goal? To use machine learning to turn volatility into victory.\n",
    "- Will it work? Well, let’s just say our fallback plan involves a lot of ramen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your own imports here\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Do Not Edit Below the lines\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "from utilities.api import *\n",
    "from utilities.common import *\n",
    "from utilities.data_utils import *\n",
    "from utilities.classify import *\n",
    "from utilities.db_tools import conn\n",
    "import tensorflow.config as tfc # type: ignore\n",
    "from transformers import pipeline # type: ignore\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We're just setting the GPU here, if you don't have one... no worries, I don't either.\n",
    "It will fall back to your CPU :)\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# device = set_device(use_gpu=True)\n",
    "# print(f\"Using {device}\")\n",
    "if tfc.list_physical_devices('GPU'):\n",
    "    device = 0  # Use the first GPU\n",
    "else:\n",
    "    device = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load your environmental variables. Define a few others.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "api_key = os.getenv('NEWS_API_KEY')\n",
    "base_dir = os.getenv('ROOT_DIR')\n",
    "project_name = 'adta_5410'\n",
    "version = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set environment variable: CHARTS_DIR=adta_5410_v1_2024-10-15/charts\n",
      "Set environment variable: DATA_DIR=adta_5410_v1_2024-10-15/data\n",
      "Set environment variable: MODELS_DIR=adta_5410_v1_2024-10-15/models\n",
      "Set environment variable: PIPELINE_DIR=adta_5410_v1_2024-10-15/pipeline\n",
      "Set environment variable: LOGS_DIR=adta_5410_v1_2024-10-15/logs\n",
      ".env_temp file created at: adta_5410_v1_2024-10-15/.env_temp\n",
      "Set environment variable: PROJECT_DIR=adta_5410_v1_2024-10-15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a project directory and subfolders\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "project_path = create_project(project_name, version)\n",
    "\n",
    "# Load the project-specific environment variables from .env_temp\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(os.path.join(project_path, \".env_temp\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to pick a company in the S&P 500 to train our model on.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "eval_start = '2024-09-16'\n",
    "eval_stop = '2024-10-10'\n",
    "company = 'Tesla'\n",
    "ticker = 'TSLA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function gets the news. There are limitations. Check out the README.md. \n",
    "If you are getting a \"Error fetching data: 426 Client Error: Upgrade Required for url\",\n",
    "It is because of the developer license limitations. Adjust your eval_start in the cell above.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "articles = get_news(api_key, company, eval_start, eval_stop)\n",
    "# for article in articles:\n",
    "#     print(f\"Title: {article['title']}\")\n",
    "#     print(f\"Published: {article['publishedAt']}\")\n",
    "#     print(f\"Content: {article['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is where we define our pre-trained model. The one below is just a place holder.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Need someone to train this model\n",
    "classifier = pipeline(\"zero-shot-classification\", \n",
    "                      model=\"facebook/bart-large-mnli\",\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is where we define our pre-trained model. The one below is just a place holder.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Need someone to train this model\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                                device=device\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 28 financial articles.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is where we call the classifier model to filter out the financial news from the garbage.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# articles.drop(columns='Unnamed: 0')\n",
    "filtered_articles = [\n",
    "        article for article in articles if is_financial_article(article, classifier)\n",
    "    ]\n",
    "print(f\"Filtered {len(filtered_articles)} financial articles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Put the article data in a dataframe\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "data = []\n",
    "for article in articles:\n",
    "    sentiment = article_sentiment(article, sentiment_classifier)\n",
    "    data.append({\n",
    "        'date': article.get('publishedAt', ''),\n",
    "        'title': article['title'],\n",
    "        'content': article.get('content', ''),\n",
    "        'sentiment_score': sentiment    \n",
    "    })\n",
    "articles = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we use our trusty yfinance module to get stocks. The variables are already defined above.\n",
    "Notice we are pulling the same date ranges as the news articles.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "stocks = get_stocks(ticker, eval_start, eval_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We are going to join the two dataframes on \"date\" column. In order to do that,\n",
    "we need the datetime formats to match. Basically, it converts the column names in\n",
    "the dataframes to match, reformats the datetime, then trims off the time component.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "df_stocks = format_date(stocks)\n",
    "df_articles = format_date(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Saving ourselves some heart ache later on.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "df_stocks = format_column_names(df_stocks)\n",
    "df_articles = format_column_names(df_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Okay, here we merge everything on date. The fillna is there just in case one of the api's \n",
    "returns a null.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "merged_data = pd.merge(df_stocks, df_articles, on='date', how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we drop some columns we don't plan on using. Regression models aren't fans of non-numerical data.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "merged_data.drop(columns=['title','content'], axis=1, inplace=True)\n",
    "merged_data['sentiment_count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to aggregate the sentiment scores for the weekends and apply them to the following monday.\n",
    "Weekend news can impact opening trades on Monday. TODO: Work in Progress Weekends and Holidays\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# weekend_df = handle_weekends(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is defining a dictionary structure that I'll pass to a function. On some days, you'll get several news articles.\n",
    "Our prediction is based on the aggregation of all financial news per day.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "agg_columns = {\n",
    "    'open': 'mean',\n",
    "    'high': 'mean',\n",
    "    'low': 'mean',\n",
    "    'close': 'mean',\n",
    "    'adj_close': 'mean',\n",
    "    'volume': 'mean',  \n",
    "    'sentiment_score': 'mean', \n",
    "    'sentiment_count': 'count' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We pass the dict along with the dataframe to the function. Your output should be 1 row per day.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "df_aggregated = aggregate_column(merged_data, agg_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully at adta_5410_v1_2024-10-15/data/Tesla_articles.csv\n",
      "File saved successfully at adta_5410_v1_2024-10-15/data/Tesla_stocks.csv\n",
      "File saved successfully at adta_5410_v1_2024-10-15/data/Tesla_merged.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "save files as needed\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "filename = f'{company}'\n",
    "filepath = os.getenv('DATA_DIR')\n",
    "df_sentiment_saved = save_data(df_articles, filename+'_articles', filepath, xlsx=False)\n",
    "df_stock_saved = save_data(df_stocks, filename+'_stocks', filepath, xlsx=False)\n",
    "df_final_saved = save_data(df_aggregated, filename+'_merged', filepath, xlsx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'training_data' is ready.\n",
      "Inserted batch 1 successfully\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Save the DataFranme into the database for later processign.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "df_to_postgres(df_aggregated, 'training_data', conn)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
