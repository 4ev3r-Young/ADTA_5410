{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://webassets.unt.edu/assets/branding/unt-stacked-logo.svg\" alt=\"UNT | University of North Texas\" class=\"desktop-logo\" width=\"300\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; justify-content: space-around; padding: 20px 40px 20px 20px; background-color: #f4f4f9; border-radius: 10px;\">\n",
    "  <!-- Team Member 1 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Sonali Sabnam</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Boss</p>\n",
    "  </div>\n",
    "  \n",
    "  <!-- Team Member 2 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Sonam Pohuja</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Other Boss</p>\n",
    "  </div>\n",
    "\n",
    "  <!-- Team Member 3 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Luis Garcia Fuentes</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Cool Guy</p>\n",
    "  </div>\n",
    "\n",
    "  <!-- Team Member 4 -->\n",
    "  <div style=\"text-align: center; background-color: #ffffff; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); width: 150px;\">\n",
    "    <img src=\"https://via.placeholder.com/100\" style=\"border-radius: 50%; width: 100px; height: 100px;\">\n",
    "    <h3 style=\"font-family: Arial, sans-serif; color: #333;\">Young Yu</h3>\n",
    "    <p style=\"font-family: Arial, sans-serif; color: #666;\">Da Janitor</p>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to the Futures: The AI Stock Predictor\n",
    "Back to the Futures is a project where we try to do the impossible—predict the stock market with AI! <br>\n",
    "Because if there’s one thing the stock market loves, it’s being perfectly predictable.  <br>\n",
    "(spoiler: it’s not). <br>\n",
    "- Our goal? To use machine learning to turn volatility into victory.\n",
    "- Will it work? Well, let’s just say our fallback plan involves a lot of ramen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your own imports here\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Do Not Edit Below the lines\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "from utilities.api import *\n",
    "from utilities.classify import *\n",
    "from utilities.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Put your secrets here.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We're just setting the GPU here, if you don't have one... no worries, I don't either.\n",
    "It will fall back to your CPU :)\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "device = set_device(use_gpu=True)\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is where we define our pre-trained model. The one below is just a place holder.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Need someone to train this model\n",
    "classifier = pipeline(\"zero-shot-classification\", \n",
    "                      model=\"facebook/bart-large-mnli\",\n",
    "                      device=device\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is where we define our pre-trained model. The one below is just a place holder.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# Need someone to train this model\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", \n",
    "                                model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "                                device=device\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We need to pick a company in the S&P 500 to train our model on.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "eval_start = '2024-09-11'\n",
    "eval_stop = '2024-09-15'\n",
    "company = 'Tesla'\n",
    "ticker = 'TSLA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function gets the news. There are limitations. Check out the README.md\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "articles = get_news(api_key, company, eval_start, eval_stop)\n",
    "# for article in articles:\n",
    "#     print(f\"Title: {article['title']}\")\n",
    "#     print(f\"Published: {article['publishedAt']}\")\n",
    "#     print(f\"Content: {article['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is where we call the classifier model to filter out the financial news from the garbage.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "filtered_df = [article for article in articles if is_financial_article(article, classifier)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We'll take the output of the classifier above and classify it as Positive, Neutral, or Negative.\n",
    "In other words, 1,0, -1. This also gives a confidence score that we'll use as a weight to multiply\n",
    "against the sentiment. For example, -1 (bad news) * 0.97 (This how confident the sentiment model is that it is bad news).\n",
    "So, we get a -0.97 final score.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "data = []\n",
    "for article in articles:\n",
    "    sentiment = article_sentiment(article, sentiment_classifier)\n",
    "    data.append({\n",
    "        'date': article.get('publishedAt', ''),\n",
    "        'title': article['title'],\n",
    "        'content': article.get('content', ''),\n",
    "        'sentiment_score': sentiment    \n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "sentiment_df = pd.DataFrame(data)\n",
    "\n",
    "# Print the sentiment results\n",
    "# for article, sentiment in zip(articles, sentiment_results):\n",
    "#     print(f\"Article Title: {article['title']}\")\n",
    "#     print(f\"Weighted Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we use our trusty yfinance module to get stocks. The variables are already defined above.\n",
    "Notice we are pulling the same date ranges as the news articles.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "stock_df = get_stocks(ticker, eval_start, eval_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We are going to join the two dataframes on \"date\" column. In order to do that,\n",
    "we need the datetime formats to match. Basically, it converts the column names in\n",
    "the dataframes to match, reformats the datetime, then trims off the time component.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "df_stocks = format_date(stock_df)\n",
    "df_sentiment = format_date(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Okay, here we merge everything on date. The fillna is there just in case one of the api's \n",
    "returns a null.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "merged_data = pd.merge(df_stocks, df_sentiment, on='date', how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we drop some columns we don't plan on using. Regression models aren't fans on non-numerical data.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "merged_data.drop(columns=['title','content'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is defining a dictionary structure that I'll pass to a function. On some days, you'll get several news articles.\n",
    "Our prediction is based on the aggregation of all financial news per day.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "agg_columns = {\n",
    "    'Open': 'mean',\n",
    "    'High': 'mean',\n",
    "    'Low': 'mean',\n",
    "    'Close': 'mean',\n",
    "    'Adj Close': 'mean',\n",
    "    'Volume': 'mean',  \n",
    "    'sentiment_score': 'mean', \n",
    "    'sentiment_score': 'count' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We pass the dict along with the dataframe to the function. Your output should be 1 row per day.\n",
    "'''\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "aggregated_df = aggregate_column(merged_data, agg_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert stock data to DataFrame\n",
    "# stock_df = pd.DataFrame(stock_data)\n",
    "\n",
    "# # Convert article sentiment data to DataFrame\n",
    "# sentiment_df = pd.DataFrame(article_sentiment_data)\n",
    "# sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "# # Aggregate sentiment by date (e.g., take the average sentiment per day)\n",
    "# aggregated_sentiment = sentiment_df.groupby('date')['sentiment'].mean().reset_index()\n",
    "\n",
    "# # Merge stock data with aggregated sentiment data\n",
    "# df = pd.merge(stock_df, aggregated_sentiment, on='date', how='left')\n",
    "\n",
    "# # Fill in article_count to ensure it matches\n",
    "# df['article_count'] = df['article_count'].fillna(0)\n",
    "\n",
    "# # Add cumulative factor for days without news\n",
    "# df['cumulative_sentiment'] = df['sentiment']\n",
    "# days_since_news = 0\n",
    "\n",
    "# for i in range(1, len(df)):\n",
    "#     if np.isnan(df.loc[i, 'sentiment']):\n",
    "#         # No news, carry forward the last sentiment and increase its weight\n",
    "#         df.loc[i, 'cumulative_sentiment'] = df.loc[i-1, 'cumulative_sentiment'] * (1 + days_since_news)\n",
    "#         days_since_news += 1\n",
    "#     else:\n",
    "#         # Reset days since news if there's new news\n",
    "#         days_since_news = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # a simple LSTM model\n",
    "# class StockPredictor(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(StockPredictor, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size=2, hidden_size=64, batch_first=True)\n",
    "#         self.fc = nn.Linear(64, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         output = self.fc(lstm_out[:, -1, :])  # Predict using the last time step's output\n",
    "#         return output\n",
    "\n",
    "# # Instantiate and train model \n",
    "# # need to decide on an optimizer and loss function\n",
    "# model = StockPredictor()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/transformers/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
